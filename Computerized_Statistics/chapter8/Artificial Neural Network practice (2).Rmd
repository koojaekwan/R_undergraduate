---
title: ""
author: "Jae Kwan Koo"
output:
  github_document:
    toc: yes
    toc_depth: 4
  word_document: default
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  


```{r}
library(nnet)
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')

library(clusterGeneration)
library(scales)
library(reshape)
```  

`nnet` : 순전파 알고리즘에 필요한 패키지.  


## IRIS data - forward propagation  

```{r}
nn.iris <- nnet(Species~., data=iris, size=2, rang=.1, decay=5e-4, maxit=1000)
summary(nn.iris)
```  

4가지 변수 값을 이용해 종류를 분류해보려고 한다.
size=2 : 은닉층이 1개이고 은닉노드가 2개인 순전파모형.  
4-2-3구조를 가지고 있는 신경망.  
모든 선들을 연결한다면 이 신호의 강도는 가중치이다. 이 가중치는 19개이다. 이 19개를 다 추정해야한다.  
활성함수는 여러가지이다. 여기서 softmax modelling이다. 이것은 확장된 형태이고 우리는 시그모이드를 이야기 했었다.  
w0의 의미를 가지는 것이 b이다. 절편의 역할을 한다고 볼 수 있다.  

```{r}
plot.nnet(nn.iris)    # plot(nn.iris)
```  

### confusion matrix  

```{r}
confusion_matrix<-table(iris$Species, predict(nn.iris, iris, type='class'))
```  

반응변수가 범주형인 경우이므로 type='class' 옵션을 사용.  
~~그냥 단순한 예제로 여기서는 train, test data set은 나누지 않고 실습하였다.~~  


```{r}
round(prop.table(confusion_matrix) * 100, digit = 1)

sum(diag(round(prop.table(confusion_matrix) * 100, digit = 1)))
```  

또한, percent로 나타내어 정분류율, 오분류율도 확인할 수 있을 것이다.  
정분류율은 위와 같다.  


## infert data - back propagation  

```{r}
library(neuralnet)

data(infert)
net.infert <- neuralnet(case~age+parity+induced+spontaneous, hidden=2, data=infert, linear.output=F)

net.infert$result.matrix
plot(net.infert)
```  

역전파 알고리즘을 사용하기 위해 neuralnet패키지를 이용한다. 반응변수가 0과 1만 가지는 범주형이기 때문에, linear.output옵션에서 F로 둔 것을 알 수 있다.  
은닉층이 1개 은닉노드가 2개임을 hidden을 통해 알 수 있다.  

infert 데이터는 유아기 자녀들 248명을 대상으로 여러가지 물어보았다.
case : 반응변수  

hidden은 nnet에서 size와 동일하다. 은닉층, 은닉노드를 나타낸다.  
linear.output : 인공신경망 모형을 수치이면 예측모형으로 사용가능하기 때문에 출력되는 결과가 수치인지 물어보는 것이다 T라고 두면 예측모형으로 보게 된다. 여기서는 여부를 나타내는 범주자료를 반응변수로 하게 되는 분류모형이기 때문에 F로 두자.  

```{r}
str(infert)
```  

```{r}
net.case <- compute(net.infert, infert)
results <- data.frame(actual = infert$case, prediction = net.case$net.result)

final_results <- ifelse(results>=0.5,1,0)

(pred<-table(actual = final_results[,1],prediction = final_results[,2]))
```  

```{r}
sum(diag(pred))/sum(pred)
```  

정분류율은 약 76.6%이다.  

### GW Plot  

```{r}
par(mfrow=c(2,2))
gwplot(net.infert, selected.covariate='age', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='parity', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='induced', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='spontaneous', min=-2.5, max=5)
```  

gw는 generalize weight이다. 회귀모형과 비교시 원인과 결과를 알기 어렵다.  
가중치를 회귀모형안에서 회귀계수처럼 이용하는 가중치를 gw라고 한다.  

gw가 0에서 크게 벗어나지 않는다 -> age의 값이 어떤값을 가지든간에 0에 거의 되어있다.  
즉, age변수같은 경우 모형 안에서의 영향력이 매우 미미하다.  

다른변수들은 gw값의 폭이 큰 폭으로 되어있다. age는 다른변수에 비해 미치는 영향이 매우작다 -> 모형에 포함시켜 모형을복잡하게 만들필요없다. age를 제외시키고 모형을 만들어봐도 괜찮지 않을까라는 생각을 해보기도 한다.  

은닉층, 은닉노드를 늘려가며 예측을 더 잘하는 모형을 만들어봐도 괜찮을것 같다.  

## Boston data  

### glm  

```{r}
library(MASS)
head(Boston)

apply(Boston,2,function(x) sum(is.na(x)))
```  

보스턴에 있는 각 지역에 평균집값에 영향주는 요인이 있을건데 어떤 영향을 주는지 보자.  
앞의 예제와는 다르게 예측모형을 만드는 것이 목표이다.  

medv는 수치자료이므로 수치자료에 대해 모형을 만들게 되면 예측모형이다. ~~(반응변수가 범주이면 분류모형)~~  
자료에 대한 점검으로 str함수와 summary함수를 이용하자. ~~(범주형일경우 summary에서 평균을 보는 의미는 없다.)~~  

apply로 각 변수별 결측 갯수를 보자.  



```{r}
index <- sample(1:nrow(Boston),round(0.75*nrow(Boston)))
train <- Boston[index,]
test <- Boston[-index,]
lm.fit <- glm(medv~., data=train)

pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)

summary(lm.fit)
```  

전체 데이터 행은 506개이므로 전체를 가지고 만든 모형을 일반적으로 생각한다.  
가장 최적의 해를 구해서 만든 모형이므로 506개에만 특화된 모형이라고 생각될 수 있다. 좋은지 나쁜지 확인하기 어렵다.  따라서 2파트로 쪼개서 나머지 하나를 모형에 대한 평가로 쓴다. 더 잘게 쪼개는 cross-validation방법도 있다.  

7:3의 비율로 뽑아낸다. 항상 7:3이 맞는 것은 아니다. 그냥 모형을 만드는 사람의 주관에 맡긴다. 대신 9:1이거나 1:9 같은 경우 한쪽에 너무 치우치게는 말고 적어도 8:2정도는 활용해야 한다. 모형을 만드는 쪽에 더 많이 할당한다.  
여기서는 75:25로 분할하였다. 이 75%를 선택할 때, 임의로 선택해야 한다. 그냥 1번부터 하게되면 일반성에 어긋나게 된다.  

각각 행의 번호를 가지고 뽑아올 것이다. 1부터 504까지 이루어져있는 벡터를 만들어 75%만큼 임의로 뽑아낸다.  

sample함수는 replace=F라는 default값을 가지고 있다.  
380개를 임의추출하여 모형을 만들고 126개를 가지고 모형을 평가하겠다.  

### back propagation  

```{r}
maxs <- apply(Boston, 2, max) 
mins <- apply(Boston, 2, min)
scaled <- as.data.frame(scale(Boston, center = mins, scale = maxs - mins))

train_ <- scaled[index,]
test_ <- scaled[-index,]

n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))

net.Boston <- neuralnet(f,data=train_,hidden=c(5,3), linear.output=T)

plot(net.Boston)
net.Boston$result.matrix
```  

신경망을 활용한 모형이다.  
아까는 원데이터를 활용했다. 원데이터를 활용하게 되면 척도가 다르다. 회귀모형에서는 척도 조정이 가능하다.  
신경망에서 스케일을 동일하게 조정하기 위해 center을 mins, scale을 max-min으로 하였다.  


 as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))와 같이 표현해줄 수 도 있다. ~~(위의 회귀식 구축할 때와 동일)~~  

linear.out=T : 반응변수가 수치이므로 예측모형을 의미.  
plot을 보면 왼편이 독립변수 오른편이 반응변수이다.  

두번째 은닉층에는 3개의 은닉노드를 만들었다. 첫번째 은닉층에는 5개의 은닉노드를 만들었다. 1이라고 되어있는 것은 결합시 절편도 필요하므로 절편을 의미한다.  

각 가중치에 대한 것도 볼 수 있고, 에러도 볼 수 있다.
디폴트로 에러를 계산하는 것은 sse이다. 0에 가까우므로 성능이 굉장히 좋은 것. 행의 갯수로 나눠주면 더 작아지므로 앞의 모형보다 좋아 보인다.  

test모형에 적용해보자 : compute라는 함수를 사용 (회귀모형에서는 predict 함수를 사용했다.)  

scale했으므로 원데이터로 다시 reverse scale해야한다.  
(pr.nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv))  

확률변수를 가정하지 않았기 때문에 x-mu /sigma대신 x-min / (max-min) 을 했었다. 다시 돌려주기 위해 max-min값을 곱하고  min을 더해서 역 스케일화 하였다.  

#### comparison  

```{r}
pr.nn <- compute(net.Boston,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
test.r <- (test_$medv)*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)

c(MSE.lm,MSE.nn)
```  

glm과 ANN의 MSE를 서로 비교할 수 있다. 신경망으로 만든 모형의 MSE가 더 작다.  

```{r}
par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)


plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=1)
points(test$medv,pr.lm,col='blue',pch=18,cex=1)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
```  

신경망 모형이 예측을 더 잘한다라고 말할 수 있다. 보통은 신경망이 예측을 더 잘한다.  

















