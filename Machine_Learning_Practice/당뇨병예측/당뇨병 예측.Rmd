---
title: "머신러닝을 이용한 당뇨병 예측"
author: "Jae Kwan Koo"
output:
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document: default
  github_document:
    toc: yes
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

```{r}
# install.packages("webshot")
# webshot::install_phantomjs()
```  




### Introduction.  

당뇨병에 영향을 주는 8가지 특징을 바탕으로, 기초적인 데이터분석과 통계학습을 통해 당뇨병을 예측하자.


변수 | 설명  
-|-|  
preg | 임신횟수  
plas | 혈장 포도당 농도  
pres | 혈압  
skin | 피부 주름 두께  
insu | 2시간 혈청 인슐린  
mass | weight / (height)^2  
pedi | 당뇨병 혈통 기능  
age  | 나이  
class | 당뇨병 여부  

### Library  

```{r library, warning=FALSE, message=FALSE}
# data wrangling
library(data.table)
library(tidyverse)    
    

# data assessment/visualizations
library(DT)       
library(corrplot)
library(knitr)
library(plotly)
library(patchwork)

# model
library(caret)
library(doParallel)    # parallel in caret
library(rpart)         # making decision tree
library(rpart.plot)    # plot for decision tree
library(randomForest)  # using randomForest algorithm
library(pROC)          # using roc function in package
library(mice)          # (multiple imputation, MI)
library(xgboost)
```  

### parallel process in caret  

```{r}
no_cores <- detectCores()-1
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl)


## When you are done:
# stopCluster(cl)
```

### Import the Data  


```{r}
data <- fread("dataset_37_diabetes.csv")
```  

## Data manipulate    

```{r, echo=FALSE}
datatable(data,caption = 'Table : whole data',
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

```{r}
summary(data); str(data)
data$class<-as.factor(data$class)
```  

### Handling the Missing Values.  


```{r}
sapply(data,function(x) sum(is.na(x)))
sapply(data,function(x) sum(x==0))
```  

결측치는 없고 0의 수치를 가진 데이터들이 많이 존재함을 일단 알 수 있다. preg를 제외한 변수들은 0을 가진다는 것이 말이 되지 않으므로 이 수치를 어떻게 대체할지 생각해보자.  

```{r}
data<-data %>% mutate(plas=replace(plas, list=(plas==0), NA),
                      pres=replace(pres, list=(pres==0), NA),
                      skin=replace(skin, list=(skin==0), NA),
                      insu=replace(insu, list=(insu==0), NA),
                      mass=replace(mass, list=(mass==0), NA))


# dataset을 m개 만든다. 디폴트는 5 , method : randomforest
miceMod <- mice(data, method="rf",seed = 1234, print=F, m = 5) 


miceOutput <- complete(miceMod, 1)
datatable(miceOutput)
```  

complete함수를 통해 결측값에 대해 채워진 dataframe을 재지정.  
이 채워진 데이터들에 대해 회귀분석을 해보자.  

```{r}
fit <- with(miceMod,lm(preg~plas+pres+skin+insu+mass+pedi+age+class))

pool_fit <- pool(fit)
summary(pool_fit)
```  

모든 변수들에 대해 낮은 유의확률에도 유의하다. 따라서 이 mice패키지를 이용해 채운 데이터를 사용해보자.   

```{r}
anyNA(miceOutput)
str(miceOutput)

data2 <- miceOutput  # not exist NA value in data
```  

결측치는 없음을 확인했고, preg(임신횟수)를 제외한 나머지 변수에서 0은 일어날 수 없는 수치이므로 mice패키지를 이용하여 결측치를 채워넣었다.  

* Refer  

[mice1](https://rstudio-pubs-static.s3.amazonaws.com/192402_012091b9adac42dbbd22c4d07cb00d36.html)  

[mice2](https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html)  

### making age group.  

나이대에 대한 변수를 새로 만들었다.  

```{r}
data2 %>% mutate(age_group=
                  case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 & age<70 ~ "60s",
                            age>=70 & age<80 ~ "70s",
                            age>=80 & age<90 ~ "80s")) %>% 
  group_by(age_group) %>% 
  summarise(n=n())
```  



```{r, message=FALSE, warning=FALSE, echo=TRUE}
# group_num <- data2 %>% mutate(age_group=
#                                 case_when(age>=20 & age<30 ~ "20s",
#                                           age>=30 & age<40 ~ "30s",
#                                           age>=40 & age<50 ~ "40s",
#                                           age>=50 & age<60 ~ "50s",
#                                           age>=60 ~ "60~")) %>%
#   group_by(age_group) %>%
#   summarise(number=n()) %>%
#   ggplot(aes(x=age_group,y=number))+geom_bar(stat="identity")+
#   theme_bw(base_size = 20,base_line_size = 0.5)
#
# ggplotly(group_num,session="knitr",width = 500, height = 500)
```  
  

```{r mean table}
mean_group <- data2 %>% mutate(age_group=
                            case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 ~ "60~")) %>% group_by(age_group) %>%
  summarise(mean(preg),mean(plas),mean(pres),mean(skin),mean(insu),mean(mass),mean(pedi))



knitr::kable(cbind(mean_group[,1],round(mean_group[,-1],3)))   # table form
```  

나이대별로 각 요인들의 평균치이다.  

```{r}
data3 <- data2 %>% mutate(age_group=
                            case_when(age>=20 & age<30 ~ "20s",
                                      age>=30 & age<40 ~ "30s",
                                      age>=40 & age<50 ~ "40s",
                                      age>=50 & age<60 ~ "50s",
                                      age>=60 ~ "60~"))
```  

age group변수를 추가한 data3를 이제 사용하겠다.  


## EDA  

### Box-Plot  

```{r, message=FALSE, warning=FALSE, echo=TRUE, fig.height=9, fig.width=8}
p1 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=preg))+ geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")  

p2 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=plas))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")  

p3 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=pres))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")

p4 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=skin))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue") 

p5 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=insu))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")

p6 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=mass))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")

p7 <- 
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=pedi))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun="mean", geom="point", shape=22, size=3, fill="blue")


(p1+p2) / (p3+p4) / (p5+p6) / p7

# p1+p1+p3+p4+p5+p6+p7 + plot_layout(ncol=2)
```  

### class_level variable  

```{r echo=FALSE}
data3$class_level <- ifelse(data3$class=="tested_negative",0,1)

datatable(data3,caption = 'Table : adjusted whole data',
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

```{r}
data3 %>% group_by(age_group) %>% 
  summarise(ratio=sum(class_level)/n()) %>%
  ggplot(aes(x=age_group,y=ratio))+geom_bar(stat="identity")
```  

당뇨병 환자들의 나이대들에 대한 비율 그래프이다. 위의 boxplot에서 plas(혈장 포도당 농도)의 그래프와 비슷한 양상을 보이고 있다. ~~preg, pres가 당뇨병과 밀접한 관계가 있지 않을까?~~  

### Correlation Plot.  

```{r}
data3 %>% 
  select(-class, -age_group, -class_level) %>%
  select_if(is.numeric) %>%
  cor(use="complete.obs") %>%
  corrplot.mixed(tl.cex=0.85)
```  

(preg, age) (plas, insu) (skin, mass) 변수들이 어느정도 상관관계를 보이고 있다.  

## Machine Learning Algorithm. {.tabset}  

```{r}
set.seed(1234)
index<-createDataPartition(data3$class,p=0.7,list=F)

train<-data3[index,]
test<-data3[-index,]
```  

train과 test데이터를 70%확률로 랜덤하게 분리.  



```{r echo=FALSE}
# datatable(train,caption = 'Table : train data',
#           filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
# 
# datatable(test, caption = "Table : test data",
#           filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

### Logistic Regression.  

```{r}
str(data3)
set.seed(1234)

logistic_model<-glm(class~preg+plas+pres+skin+insu+mass+pedi+age,
                    data = train, family = "binomial")

summary(logistic_model)
```  

```{r}
test$class_prob<-predict(logistic_model, test[,1:8],type = "response")

test$class_pred<-ifelse(test$class_prob>0.5,1,0)
```  

response로 두어 link에 걸리는 y의 확률값을 계산  

```{r}
mean(test$class_level==test$class_pred)

confusionMatrix(as.factor(test$class_pred), as.factor(test$class_level))
```  


```{r}
# In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of class_level you built earlier.  

ROC<-roc(test$class_level, test$class_prob)           # Create a ROC curve

plot(ROC,col="blue")                                  # Plot the ROC curve
auc(ROC)                                              # Calculate the area under the curve (AUC)
```  

The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). Classifiers that give curves closer to the top-left corner indicate a better performance.  

* refer  

[ROC1](https://nittaku.tistory.com/297)  

[ROC2](https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/)  



### Decision Tree.  

```{r}
# tree_model <- rpart(class~preg+plas+pres+skin+insu+mass+pedi+age, data = train, method = "class", control = rpart.control(cp = 0))
# 
# tree_model

# Fit the model on the training set
set.seed(1234)

tree_model <- train(class~preg+plas+pres+skin+insu+mass+pedi+age, 
                    data = train,
                    method = "rpart",
                    trControl = trainControl("cv", number = 5),
                    tuneLength = 10)

# Plot model accuracy vs different values of
# cp (complexity parameter)
plot(tree_model)
```  

```{r}
# Print the best tuning parameter cp that
# maximizes the model accuracy
tree_model$bestTune
```  

```{r}
# Plot the final tree model
# par(xpd = NA) # Avoid clipping the text in some device
# 
# plot(tree_model$finalModel)
# text(tree_model$finalModel,  digits = 3)
```  

```{r}
# Decision rules in the model
tree_model$finalModel
```


```{r}
prediction <- predict(tree_model, test[,1:8])

table(actual = test$class,pre=prediction); mean(prediction==test$class)
confusionMatrix(prediction,test$class)
```  

```{r}
# Plot the model with default settings
# rpart.plot(tree_model)
```  

```{r}
# cross-validation을 계산해 주는 함수로 print.cp를 제공,
# rpart패키지도 과적합화 문제가 있기 때문

# printcp(tree_model)
# plotcp(tree_model)
```  

```{r}
# ptree <- prune(tree_model, cp= tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"])
# plot(ptree)
# text(ptree)
```  

<!-- 가지치기를 통해 과적합을 어느정도 줄인뒤, 다시 성능을 확인해보자.   -->

```{r}
rpartpred <- predict(tree_model, test, type='raw')

confusionMatrix(rpartpred, test$class)
```  


### RandomForest  

```{r}
set.seed(1234)

RandomForest_model <- randomForest(class~preg+plas+skin+insu+mass+pedi+age,
                                   data=train[,1:9],ntree=500,importance=T)

prediction2 <- predict(RandomForest_model, test[,1:8], type = "class")
mean(prediction2==test$class)

confusionMatrix(prediction2, test$class)
```  

```{r}
importance(RandomForest_model)
varImpPlot(RandomForest_model, type=2, pch=19, col=1, cex=1, main="")
```  



```{r}
importance_data <- data.frame(importance(RandomForest_model))
importance_data[,5] <- rownames(importance_data)

ggplot(data=importance_data,aes(x=reorder(V5,-MeanDecreaseGini),y=MeanDecreaseGini))+
  geom_bar(stat="identity",fill="red")+
  theme_bw()
```  

변수의 중요도를 확인할 수 있다. 중요도가 낮은 하위 2개의 변수를 제외하고 다시 모델링을 해보자.  


```{r}
set.seed(1234)

RandomForest_model2 <-
  randomForest(class~plas+insu+mass+pedi+age,
               data=train[,1:9], ntree=500, importance=T)

prediction3 <- predict(RandomForest_model2, test[,1:8], type = "class")


mean(prediction3==test$class)
confusionMatrix(prediction3, test$class)
```  

성능이 개선된 것을 확인할 수 있다.  

```{r}
plot(RandomForest_model2$err.rate[,1], col='red')
```  

#### Random forest with cross validation in caret  


```{r}
control <- trainControl(method="repeatedcv", 
                        number = 10, 
                        repeats = 5,
                        index = createMultiFolds(train$class, k=5, 
                                                 times = 5))

customGrid <- expand.grid(mtry = 1:20)



rf <- train(x = train[,1:8], y = train$class, 
            method = "rf", 
            importance=TRUE,
            trControl = control,
            tuneGrid = customGrid,
            verbose = F,
            preProcess = c("center", "scale"))


plot(rf)

pre <- predict(rf, test[,1:8])

confusionMatrix(pre, test$class)
```  

### XGBOOST  

```{r}
input_x <- as.matrix(train[,1:8])
input_y <- train$class_level
```  

matrix형태로 변환  

```{r}
nrounds <- 1000


# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

plot(xgb_tune)
```  

나머지 parameter에 대해 탐색  

```{r}
tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025),
  max_depth = c(2),
  gamma = 0:10,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

plot(xgb_tune)
```  

gamma에 대해서도 탐색  

```{r}
grid_default <- expand.grid(
  nrounds = 200,
  max_depth = 2,
  eta = 0.025,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "none",
  verboseIter = T, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = input_x,
  y = input_y,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
```  

parameter결정 후, 모형 적합  

```{r}
result_xg <- ifelse(predict(xgb_tune, test)>0.5, "tested_positive", "tested_negative")

confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)
```  


## naive bayes  

```{r}
set.seed(100)

train.control <- trainControl(method = "cv", number = 10)

search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)


model_cv1 <- train(x=train[, 1:8], 
                  y=train$class, 
                  method = "nb",
                  trControl = train.control,
                  tuneGrid = search_grid,
                  preProc = c("BoxCox", "center"))

trellis.par.set(caretTheme())
plot(model_cv1)  
```  

```{r}
result_nb <- predict(model_cv1, test)

confusionMatrix(result_nb, test$class)
```



##

content below tabbed region  





## Evaluate Machine Learning Algorithms  

Method | Accuracy Performance  
-|-|  
Logistic Regression | 0.7652174  
Decision Tree | 0.7217391  
Random Forest | `r confusionMatrix(pre, test$class)[[3]][1]`  
naive bayes | `r confusionMatrix(result_nb, test$class)[[3]][1]`  
XGBOOST | `r mean(result_xg==test$class)`  


XGBOOST works good with `r mean(result_xg==test$class)*100`% accuracy.  


<br>
<br>

## F1 score  

but, we must think the measure f1 score because data is imbalanced.  
Accuracy works well on balanced data  

f1 score is harmonic mean of recall and precision  

조화평균은 단순하게 평균을 구하는 것이 아니라, 뭔가 큰 값이 있다면 페널티를 주어, 작은 값 위주로 평균을 구하게 된다.  
이런 원리로, imbalanced data에 대해서 큰 값을 가진 class가 있더라도, 강력한 효과를 통해 평균을 구하게 된다.  

$$
F1 Score = 2\times {precision\times Recall \over Precision +Recall}
$$  

Precision = TP / (TP + FP)  
Recall = TP / (TP + FN)  


```{r}
# rf
precision_rf<-
  confusionMatrix(pre, test$class)[[2]][2,2] / 
  (confusionMatrix(pre, test$class)[[2]][2,2] + confusionMatrix(pre, test$class)[[2]][1,2])

recall_rf <- 
  confusionMatrix(pre, test$class)[[2]][2,2] / 
  (confusionMatrix(pre, test$class)[[2]][2,2] + confusionMatrix(pre, test$class)[[2]][2,1])


f1_rf <- 2*((precision_rf*recall_rf)/(precision_rf+recall_rf))

# xgboost
precision_xg<-
  confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)[[2]][2,2]/
  (confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)
[[2]][2,2] + confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)
[[2]][1,2])

recall_xg <- 
  confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)[[2]][2,2]/ 
  (confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)
[[2]][2,2] + confusionMatrix(result_xg %>% as.factor, test$class %>% as.factor)
[[2]][2,1])


f1_xg <- 2*((precision_xg*recall_xg)/(precision_xg+recall_xg))




# naive bayes
precision_nb<-
  confusionMatrix(result_nb, test$class)[[2]][2,2] / 
  (confusionMatrix(result_nb, test$class)[[2]][2,2] + confusionMatrix(result_nb, test$class)[[2]][1,2])

recall_nb <- 
  confusionMatrix(result_nb, test$class)[[2]][2,2] / 
  (confusionMatrix(result_nb, test$class)[[2]][2,2] + confusionMatrix(result_nb, test$class)[[2]][2,1])


f1_nb <- 2*((precision_nb*recall_nb)/(precision_nb+recall_nb))
```  


#### Comparison  

```{r}
data.frame(f1_rf, f1_xg, f1_nb)
```  

nb가 accuracy는 조금 더 낮을지 몰라도 tested_positive를 조금 더 세밀하게 분류하여 전반적인 모형의 성능이 좋게 나온듯 싶다.  

다음에 시간이 나면, 더 나은 방법이 있는지 업데이트 할 예정이다.  















