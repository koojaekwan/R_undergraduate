---
title: "Regression Analysis ( Ⅱ ) Project 1 using R."
subtitle : "response surface anlysis, Weighted LSE, BOX-COX transformation"
author: "Jae Kwan Koo"
output:
  github_document:
    toc: yes
    toc_depth: 4
  word_document: default
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

## 1.Make your own dataset based on data in Example 6.5 (p. 251).  Let X1<-X1+e, X2<-X2+e, Y<-Y+e, where e~N(0,1^2). Do the response surface analysis with contour plot.  

### set the data
```{r}
x1<-c(4,20,12,12,12,12,12,6.3,6.3,17.7,17.7)
x2<-c(250,250,250,250,220,280,250,229,271,229,271)
y<-c(83.8,81.7,82.4,82.9,84.7,67.9,81.2,81.3,83.1,85.3,72.7)
```  

#### add the error terms.  

```{r}
set.seed(2019)
X1<-x1+rnorm(n=1,mean=0,sd=1)
X2<-x2+rnorm(n=1,mean=0,sd=1)
Y<-y+rnorm(n=1,mean=0,sd=1)

data<-data.frame(X1,X2,Y)
data
```  

### Fitting a response-surface model  

#### First-order rsm

```{r}
# install.packages("rsm")  for response-surface analysis
library(rsm)
rsm1<-rsm(Y~ FO(X1,X2),data=data)

summary(rsm1)
```  

FO는 "first-order"을 의미한다. 먼저 첫번째 반응표면모형을 적합하였다.  
아래의 분산분석표에서 lack of fit과 pure error의 분해를 확인할 수 있다. 이 예제에서 이 모형의 lack of fit의 p-value는 0.03514<alpha=0.05이다. 이 사실은 우리가 higher-order model을 사용해야 함을 제안한다.  
first-order rsm의 contour plot을 먼저 확인해본 후, 다음으로 "second-order"모형을 적합시켜보자.  

```{r}
contour(rsm1,~X1+X2, image = TRUE)
```  

#### Second-order rsm  

```{r}
rsm2<-rsm(Y~ SO(X1,X2),data=data)

summary(rsm2)
```  

second-order rsm을 수행하였다. 이제는 lack of fit이 0.088623으로 alpha(=0.05)보다 커 더이상 유의하지 않다.
따라서, 이 모형으로 반응표면분석을 수행해도 될 것 같다.  
second-order moel의 summary는 표면의 정준분석의 결과를 제공한다. 이 분석은 적합된 표면의 정상점이 코드화된 단위 (9.920527, 239.406876)로 나타나며, 실험 영역 이내에 있는 것으로 나타났다.  
또한, 둘 eigenvalue의 부호가 서로 달라 stationary point는 saddel point이다.  
contour plot을 마지막으로 확인해보자.  

```{r}
contour(rsm2,~X1+X2, image = TRUE)
```  

## 2.Make your own dataset based on data in Example 6.6(p.254).  Let X1<-X1+e, Y<-Y+e, where e~N(0,0.1^2). Compute the WLSE using the same method as the one in text.  

### set the data  

```{r}
x<-c(1.15,1.90,3,3,3,3,3,5.34,5.38,5.4,5.4,5.45,7.7,7.8,7.81,7.85,7.87,7.91,
     7.94,9.03,9.07,9.11,9.14,9.16,9.37,10.17,10.18,10.22,10.22,10.22,10.18,10.50,10.23,10.03,10.23)

y<-c(0.99,0.98,2.6,2.67,2.66,2.78,2.8,5.92,5.35,4.33,4.89,5.21,7.68,9.81,6.52,9.71,9.82,9.81,
     8.5,9.47,11.45,12.14,11.5,10.65,10.64,9.78,12.39,11.03,8,11.9,8.68,7.25,13.46,10.19,9.93)

w<-c(1.24028,2.18224,7.84930,7.84930,7.84930,7.84930,7.84930,7.43652,6.99309,6.78574,6.78574,6.30514,0.89204,0.84420,0.83963,0.82171,0.81296,0.79588,
     0.78342,0.47385,0.46621,0.45878,0.45327,0.44968,0.41435,0.31182,0.31079,0.30672,0.30672,0.30672,0.31079,0.28033,0.30571,0.32680,0.30571)
```  

35개의 관측치로 이루어진 자료이다. 여기서 w는 weight를 의미한다. 

### add the error terms.  

```{r}
set.seed(2019)
X<-x+rnorm(n=1,mean=0,sd=1)
Y<-y+rnorm(n=1,mean=0,sd=1)

data2<-data.frame(X,Y,w)
head(data2)
```  

오차항을 넣어준 뒤 X,Y의 변수를 재할당하였다. 그리고 데이터프레임으로 만들어 앞 6개 행만 확인해보았다.  

### linear model with weight  

```{r}
lm(y~x, data=data2,weights = w)
```  

오차항이 없을때, weight가 주어진 linear model이다. 이 식은 책의 식과 같음을 확인할 수 있다.  
이제 오차항이 주어졌을 때의 모형을 살펴보자.  

```{r}
lm(Y~X, data=data2, weights = w)
```  

weighted least estimator은 다음과 같다. 모형이 적합한지 확인해보자.   

$\hat{\beta_0} = -2.26413 , \hat{\beta_1} = 1.16482$  


```{r}
summary(lm(Y~X, data=data2, weights = w))
```  

절편과 기울기의 p-value는 모두 충분히 작다. 또한 F-statistic은 매우 크고 p-value는 거의 0이므로 모형은 적절하다고 판단할 수 있다.  

## 3.Make your own dataset based on data in Example 6.7(p.259).  Let Y<-Y+e, where e~N(0,1).  

### (1)Fit the data to the multiple linear regression model.  

#### set the data  

```{r}
y<-c(26,38,50,76,108,157,
     17,26,37,53,83,124,
     13,20,27,37,57,87,
     NA,15,22,27,41,63)
x1<-c(rep(0,6),rep(10,6),rep(20,6),rep(30,6))
x2<-c(rep(seq(0,60,12),4))
```  

#### add the error terms  

```{r}
set.seed(2019)
Y<-y+rnorm(n=1,mean=0,sd=1)

data3<-data.frame(x1,x2,Y)
data3
```  

y에 error terms을 더한 후 x1, x2와 함께 데이터프레임으로 만들었다. 특이사항으로는 Y값에 결측치가 한개 보인다는 것이다.   

```{r}
multi_lm<-lm(Y~x1+x2,data=data3)
summary(multi_lm)
```  

multiple linear regression model을 적합시켰다. 그 후, adjusted r square을 확인해보았다.  
F-statistic이 충분히 크고 p-value는 0에 가까워 모형은 잘 적합이 되었음을 알 수 있고, 수정된 결정계수는 0.8673이다.  

### (2)Fit the data to the Box-Cox transformation model.  

```{r}
library(MASS) #using boxcox function in r
box_cox<-boxcox(multi_lm)  #log transformation

lambda<-box_cox$x
likeli_value<-box_cox$y

order_table<-cbind(lambda,likeli_value)
sorted<-order_table[order(-likeli_value),] 
```  

boxcox함수를 통해 람다가 언제일 때 최대가능도 함수가 최대가 되는지 그림으로 살펴보았다. 대략 0 근처인 것 같다. 자세한 람다값을 살펴보자. 가능도 함수의 값이 큰 순서대로 정렬을 한 뒤, 람다값을 확인해 보았다.   

```{r}
head(sorted)  # maximum when lambda is about -0.05.
```  

$\lambda = -0.06060606$ 일 때, 최대가능도 함수는 최대가 된다. 이 값은 거의 0에 가까우므로 log transformation을 이용해보도록 한다.  

#### log transformation model

```{r}
log_trans<-lm(log(Y)~x1+x2,data=data3)
summary(log_trans)
```  

먼저, F-statistic이 충분히 크고, p-value가 거의 0이므로 모형은 적절하다고 할 수 있다. 또한 수정된  결정계수는 0.9943으로 변환하기 전보다 높다. 변환시키지 않은 원래의 모형이 box-cox 변환모형보다 훨씬 적합도가 떨어짐을 알 수 있다.  

### (3)Compare two models in (1) and (2) by using the Q-Q plot of residuals in each model.  

```{r}
library(car) #using qqPlot function.
qqPlot(rstandard(multi_lm))
qqPlot(rstandard(log_trans))
```  

변환하기 전의 Q-Q plot은 6번 잔차가 신뢰대 밖으로 나가는 모습을 보이고 있다.  
또한, 로그변환 한 모형의 Q-Q plot의 잔차들이 조금더 직선위에 잘 모여있는 모습이다. 그러므로, 로그변환한 모형이 정규분포에 더 가까움을 알 수 있다.  car package의 함수를 이용하지 않고 기본함수로도 Q-Q plot은 그려볼 수 있다.  

```{r, results="hide"}
par(mfrow=c(1,2))
qqnorm(multi_lm$residuals)
qqline(multi_lm$residuals)

qqnorm(log_trans$residuals)
qqline(log_trans$residuals)
```  

### Reference  

#### Response Surface Analysis  

[contour](https://rdrr.io/cran/rsm/man/contour.lm.html)  
[download the paper](https://www.jstatsoft.org/article/view/v032i07)  
[rsm in R for response-surface regression](https://www.rdocumentation.org/packages/rsm/versions/1.0/topics/rsm)  


#### WLSE  

[weighted linear regression in r](https://www.datasciencecentral.com/profiles/blogs/weighted-linear-regression-in-r)  


#### BOX-COX  

[box cox transformation](https://rpubs.com/bskc/288328)  


#### 참고 문헌 : 회귀분석 제 2판 (김충락, 강근석)


 



