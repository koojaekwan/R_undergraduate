---
title: "Regression Analysis ( Ⅱ ) Project 1 using R."
subtitle : "response surface anlysis, Weighted LSE, BOX-COX transformation"
author: "Jae Kwan Koo"
output:
  github_document:
    toc: yes
    toc_depth: 4
  word_document: default
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

## 1.Make your own dataset based on data in Example 6.5 (p. 251).  Let X1<-X1+e, X2<-X2+e, Y<-Y+e, where e~N(0,1^2). Do the response surface analysis with contour plot.  

### set the data
```{r}
x1<-c(4,20,12,12,12,12,12,6.3,6.3,17.7,17.7)
x2<-c(250,250,250,250,220,280,250,229,271,229,271)
y<-c(83.8,81.7,82.4,82.9,84.7,67.9,81.2,81.3,83.1,85.3,72.7)
```  

#### add the error terms.  

```{r}
set.seed(2019)
X1<-x1+rnorm(n=1,mean=0,sd=1)
X2<-x2+rnorm(n=1,mean=0,sd=1)
Y<-y+rnorm(n=1,mean=0,sd=1)

data<-data.frame(X1,X2,Y)
data
```  

### Fitting a response-surface model  

#### First-order rsm

```{r}
# install.packages("rsm")  for response-surface analysis
library(rsm)
rsm1<-rsm(Y~ FO(X1,X2),data=data)

summary(rsm1)
```  

FO는 "first-order"을 의미한다. 먼저 첫번째 반응표면모형을 적합하였다.  
아래의 분산분석표에서 lack of fit과 pure error의 분해를 확인할 수 있다. 이 예제에서 이 모형의 lack of fit의 p-value는 0.03514<alpha=0.05이다. 이 사실은 우리가 higher-order model을 사용해야 함을 제안한다.  
first-order rsm의 contour plot을 먼저 확인해본 후, 다음으로 "second-order"모형을 적합시켜보자.  

```{r}
contour(rsm1,~X1+X2, image = TRUE)
```  

#### Second-order rsm  

```{r}
rsm2<-rsm(Y~ SO(X1,X2),data=data)

summary(rsm2)
```  

second-order rsm을 수행하였다. 이제는 lack of fit이 0.088623으로 alpha(=0.05)보다 커 더이상 유의하지 않다.
따라서, 이 모형으로 반응표면분석을 수행해도 될 것 같다.  
second-order moel의 summary는 표면의 정준분석의 결과를 제공한다. 이 분석은 적합된 표면의 정상점이 코드화된 단위 (9.920527, 239.406876)로 나타나며, 실험 영역 이내에 있는 것으로 나타났다.  
또한, 둘 eigenvalue의 부호가 서로 달라 stationary point는 saddel point이다.  
contour plot을 마지막으로 확인해보자.  

```{r}
contour(rsm2,~X1+X2, image = TRUE)
```  

## 2.Make your own dataset based on data in Example 6.6(p.254).  Let X1<-X1+e, Y<-Y+e, where e~N(0,0.1^2). Compute the WLSE using the same method as the one in text.  

### set the data  

```{r}
x<-c(1.15,1.90,3,3,3,3,3,5.34,5.38,5.4,5.4,5.45,7.7,7.8,7.81,7.85,7.87,7.91,
     7.94,9.03,9.07,9.11,9.14,9.16,9.37,10.17,10.18,10.22,10.22,10.22,10.18,10.50,10.23,10.03,10.23)

y<-c(0.99,0.98,2.6,2.67,2.66,2.78,2.8,5.92,5.35,4.33,4.89,5.21,7.68,9.81,6.52,9.71,9.82,9.81,
     8.5,9.47,11.45,12.14,11.5,10.65,10.64,9.78,12.39,11.03,8,11.9,8.68,7.25,13.46,10.19,9.93)

old_w<-c(1.24028,2.18224,7.84930,7.84930,7.84930,7.84930,7.84930,7.43652,6.99309,6.78574,6.78574,6.30514,0.89204,0.84420,0.83963,0.82171,0.81296,0.79588,
     0.78342,0.47385,0.46621,0.45878,0.45327,0.44968,0.41435,0.31182,0.31079,0.30672,0.30672,0.30672,0.31079,0.28033,0.30571,0.32680,0.30571)
```  

35개의 관측치로 이루어진 자료이다. 여기서 w는 weight를 의미한다.  
선형과 비선형 least squares regression을 포함한 대부분 모델링을 처리하는 방법에서의 일반적인 가정 중 하나는 각 데이터 포인트들은 동일한 precise information을 제공한다는 것이다. 즉, 오차항의 분산은 예측 변수의 모든 값에 대해 일정하다.  
하지만, 이 가정은 모든 모형에 대해 적용하는 것이 불가능하다. 그러므로 weight가 주어진 상황에서의 분석을 해보려고 한다.  

### add the error terms.  

```{r}
set.seed(2019)
X<-x+rnorm(n=1,mean=0,sd=0.1)
Y<-y+rnorm(n=1,mean=0,sd=0.1)

data2<-data.frame(X,Y,old_w)
head(data2)
```  

오차항을 넣어준 뒤 X,Y의 변수를 재할당하였다. 그리고 데이터프레임으로 만들어 앞 6개 행만 확인해보았다.  

### Get the weights  

```{r}
g1<-c(X[1:2])
g2<-c(X[3:7])
g3<-c(X[8:12])
g4<-c(X[13:19])
g5<-c(X[20:25])
g6<-c(X[26:35])

h1<-c(Y[1:2])
h2<-c(Y[3:7])
h3<-c(Y[8:12])
h4<-c(Y[13:19])
h5<-c(Y[20:25])
h6<-c(Y[26:35])

group_data_X<-data.frame(cbind(g1,g2,g3,g4,g5,g6))
group_data_Y<-data.frame(cbind(h1,h2,h3,h4,h5,h6))
```  

X값들이 비슷한 것들끼리 그룹화 하였다. 총 6개의 그룹으로 나누었다.  

```{r}
options("scipen" = 100)     # 소수점 100자리까지 표현.

apply(group_data_X,2,mean)  # X그룹별 평균
apply(group_data_Y,2,var)   # 반응변수 Y그룹별 표본분산
```  

각 그룹내에 속하는 X값들의 표본 평균과 반응변수 값들의 표본분산을 계산하였다.  

```{r}
sj2<-c(0.00002777778, 0.00641777778,0.30577777778,1.94071555556,0.93569888889,3.89641000000)
xbarj<-c(1.598852,3.073852,5.467852,7.892852,9.196852,10.291852)

temp_data<-data.frame(temp_y=sj2,temp_x=xbarj)

temp_lm<-lm(temp_y~temp_x, data=temp_data)
summary(temp_lm)
```  

이 표본평균을 설명변수로 반응변수의 표본분산을 반응변수로 하는 적절한 회귀모형을 찾았다. p-value가 0.05보다 작아 적절한 모형이 적합되었음을 알 수 있다.   

```{r}
pre_result<-predict(temp_lm, newdata=data.frame(temp_x=data2$X))

library(tidyverse)
final_data<-data.frame(pre_result) %>% transmute(new_w = 1/pre_result) %>% cbind(data2)
final_data$new_w[1:2]<-0   # 음수의 값인 가중치에 대해 0을 부여하여 대체

final_data
```  

이 회귀식에 x_bar대신 각 관측값 $X_i$들을 대입하여 $\hat{s_i^2}$을 계산하였다. 가중치 $w_i$는 $\hat{s_i^2}$의 역수로 주어지므로 tidyverse패키지안의 dplyr패키지의 transmute함수를 이용해 전처리 했고, 기존의 데이터와 합하여 확인하였다. 음수가 나오는 weight가 있었는데 이 경우 0으로 대체한 후 진행하였다. 이를 바탕으로 새로운 weight에 대해 가중최소제곱법을 적용한 결과는 아래와 같이 주어진다.  

```{r}
lm(Y~X,data=final_data, weights=new_w)
```  

따라서 가중최소제곱법을 적용할 시에 $\hat{y} = -0.9305+1.1645X$ 이 된다.  

### linear model with weight  

```{r}
lm(y~x, data=data2,weights = old_w)
```  

오차항이 없을때, weight가 주어진 linear model이다. 이 식은 책의 식과 같음을 확인할 수 있다.  
이제 오차항이 주어졌을 때의 모형을 살펴보자.  

```{r}
summary(lm(Y~X, data=final_data, weights = new_w))
```  

절편과 기울기의 p-value는 모두 충분히 작다. 또한 F-statistic은 매우 크고 p-value는 거의 0이므로 모형은 적절하다고 판단할 수 있다.  


## 3.Make your own dataset based on data in Example 6.7(p.259).  Let Y<-Y+e, where e~N(0,1).  

### (1)Fit the data to the multiple linear regression model.  

#### set the data  

```{r}
y<-c(26,38,50,76,108,157,
     17,26,37,53,83,124,
     13,20,27,37,57,87,
     NA,15,22,27,41,63)
x1<-c(rep(0,6),rep(10,6),rep(20,6),rep(30,6))
x2<-c(rep(seq(0,60,12),4))
```  

#### add the error terms  

```{r}
set.seed(2019)
Y<-y+rnorm(n=1,mean=0,sd=1)

data3<-data.frame(x1,x2,Y)
data3
```  

y에 error terms을 더한 후 x1, x2와 함께 데이터프레임으로 만들었다. 특이사항으로는 Y값에 결측치가 한개 보인다는 것이다.   

```{r}
multi_lm<-lm(Y~x1+x2,data=data3)
summary(multi_lm)
```  

multiple linear regression model을 적합시켰다. 그 후, adjusted r square을 확인해보았다.  
F-statistic이 충분히 크고 p-value는 0에 가까워 모형은 잘 적합이 되었음을 알 수 있고, 수정된 결정계수는 0.8673이다.  

### (2)Fit the data to the Box-Cox transformation model.  

```{r}
library(MASS) #using boxcox function in r
box_cox<-boxcox(multi_lm)  #log transformation

lambda<-box_cox$x
likeli_value<-box_cox$y

order_table<-cbind(lambda,likeli_value)
sorted<-order_table[order(-likeli_value),] 
```  

boxcox함수를 통해 람다가 언제일 때 최대가능도 함수가 최대가 되는지 그림으로 살펴보았다. 대략 0 근처인 것 같다. 자세한 람다값을 살펴보자. 가능도 함수의 값이 큰 순서대로 정렬을 한 뒤, 람다값을 확인해 보았다.   

```{r}
head(sorted)  # maximum when lambda is about -0.05.
```  

$\lambda = -0.06060606$ 일 때, 최대가능도 함수는 최대가 된다. 이 값은 거의 0에 가까우므로 log transformation을 이용해보도록 한다.  

#### log transformation model

```{r}
log_trans<-lm(log(Y)~x1+x2,data=data3)
summary(log_trans)
```  

먼저, F-statistic이 충분히 크고, p-value가 거의 0이므로 모형은 적절하다고 할 수 있다. 또한 수정된  결정계수는 0.9943으로 변환하기 전보다 높다. 변환시키지 않은 원래의 모형이 box-cox 변환모형보다 훨씬 적합도가 떨어짐을 알 수 있다.  

### (3)Compare two models in (1) and (2) by using the Q-Q plot of residuals in each model.  

```{r}
library(car) #using qqPlot function.
qqPlot(rstandard(multi_lm))
qqPlot(rstandard(log_trans))
```  

변환하기 전의 Q-Q plot은 6번 잔차가 신뢰대 밖으로 나가는 모습을 보이고 있다.  
또한, 로그변환 한 모형의 Q-Q plot의 잔차들이 조금더 직선위에 잘 모여있는 모습이다. 그러므로, 로그변환한 모형이 정규분포에 더 가까움을 알 수 있다.  car package의 함수를 이용하지 않고 기본함수로도 Q-Q plot은 그려볼 수 있다.  

```{r}
par(mfrow=c(1,2))
qqnorm(multi_lm$residuals)
qqline(multi_lm$residuals)

qqnorm(log_trans$residuals)
qqline(log_trans$residuals)
```  

## Reference  - click the hyperlink!

### Response Surface Analysis  

[contour](https://rdrr.io/cran/rsm/man/contour.lm.html)  

[download the paper](https://www.jstatsoft.org/article/view/v032i07)  

[rsm in R for response-surface-analysis analysis](https://www.rdocumentation.org/packages/rsm/versions/1.0/topics/rsm)  


### WLSE  

[weighted linear regression in r](https://www.datasciencecentral.com/profiles/blogs/weighted-linear-regression-in-r)  


### BOX-COX  

[box cox transformation](https://rpubs.com/bskc/288328)  


### 참고 문헌 : 회귀분석 제 2판 (김충락, 강근석)


 



