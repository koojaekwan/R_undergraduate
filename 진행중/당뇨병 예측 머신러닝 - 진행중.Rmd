---
title: "머신러닝을 이용한 당뇨병 예측"
author: "Jae Kwan Koo"
output:
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document: default
  github_document:
    toc: yes
    toc_depth: 4
---  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, fig.align = "center", message=F, warning=F, fig.height = 5, cache=F, dpi = 300, dev = "png")
```  

```{r}
# install.packages("webshot")
# webshot::install_phantomjs()
```  

### Introduction.  

당뇨병에 영향을 주는 8가지 특징을 바탕으로, 기초적인 데이터분석과 통계학습을 통해 당뇨병을 예측하자.


변수 | 설명  
-|-|  
preg | 임신횟수  
plas | 혈장 포도당 농도  
pres | 혈압  
skin | 피부 주름 두께  
insu | 2시간 혈청 인슐린  
mass | weight / (height)^2  
pedi | 당뇨병 혈통 기능  
age  | 나이  
class | 당뇨병 여부  

```{r library}
# data wrangling
library(data.table)
library(tidyverse)    
    

# data assessment/visualizations
library(caret)         # using confusionMatrix function and createDataPartition function 
library(DT)       
library(corrplot)
library(knitr)
library(plotly)


# model
library(rpart)         # making decision tree
library(rpart.plot)    # plot for decision tree
library(randomForest)  # using randomForest algorithm
library(pROC)          # using roc function in package
library(mice)          # (multiple imputation, MI)
library(xgboost)
```  


```{r}
setwd("d:/")
data<-fread("dataset_37_diabetes.csv")
```  

## Data manupulate    

```{r, echo=FALSE}
datatable(data,caption = 'Table : whole data',
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

```{r}
summary(data); str(data)
data$class<-as.factor(data$class)
```  

### Handling the Missing Values.  


```{r}
sapply(data,function(x) sum(is.na(x)))
sapply(data,function(x) sum(x==0))
```  

결측치는 없고 0의 수치를 가진 데이터들이 많이 존재함을 일단 알 수 있다. preg를 제외한 변수들은 0을 가진다는 것이 말이 되지 않으므로 이 수치를 어떻게 대체할지 생각해보자.  

```{r}
data<-data %>% mutate(plas=replace(plas, list=(plas==0), NA),
                      pres=replace(pres, list=(pres==0), NA),
                      skin=replace(skin, list=(skin==0), NA),
                      insu=replace(insu, list=(insu==0), NA),
                      mass=replace(mass, list=(mass==0), NA))


# dataset을 m개 만든다. 디폴트는 5 , method : randomforest
miceMod <- mice(data, method="rf",seed = 1234,print=F) 


miceOutput <- complete(miceMod)
datatable(miceOutput)
```  

complete함수를 통해 결측값에 대해 채워진 dataframe을 재지정.  
이 채워진 데이터들에 대해 회귀분석을 해보자.  

```{r}
fit <- with(miceMod,lm(preg~plas+pres+skin+insu+mass+pedi+age+class))
pool_fit <- pool(fit)
summary(pool_fit)
```  

모든 변수들에 대해 낮은 유의확률에도 유의하다. 따라서 이 mice패키지를 이용해 채운 데이터를 사용해보자.   

```{r}
anyNA(miceOutput)
str(miceOutput)

data2<-miceOutput
```  

결측치는 없음을 확인했고, preg(임신횟수)를 제외한 나머지 변수에서 0은 일어날 수 없는 수치이므로 mice패키지를 이용하여 결측치를 채워넣었다.  

* Refer  
[mice1](https://rstudio-pubs-static.s3.amazonaws.com/192402_012091b9adac42dbbd22c4d07cb00d36.html)
[mice2](https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html)  

### making age group.  

분석에 따로 쓰이진 않았지만 일단 확인차 만들었다.  

```{r}
data2 %>% mutate(age_group=
                  case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 & age<70 ~ "60s",
                            age>=70 & age<80 ~ "70s",
                            age>=80 & age<90 ~ "80s")) %>% group_by(age_group) %>% summarise(n=n())
```  

나이대별 분포를 알 수 있다.  60대 이상부터는 수가 적으므로 한 그룹으로 묶는게 나아보인다.  

```{r}
data2 %>% mutate(age_group=
                  case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 ~ "60~")) %>% group_by(age_group) %>% summarise(n=n())
```  

```{r, message=FALSE, warning=FALSE, echo=TRUE,}
group_num<-data2 %>% mutate(age_group=
                  case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 ~ "60~")) %>% group_by(age_group) %>% 
  summarise(number=n()) %>% 
  ggplot(aes(x=age_group,y=number))+geom_bar(stat="identity")+
  theme_bw(base_size = 20,base_line_size = 0.5)

ggplotly(group_num,session="knitr",width = 500, height = 500)
```  

나이대별 수.  
~~마우스로 bar에 가져다대면 그에 대한 빈도수를 알 수 있고, 조정할 수 있다.(html이용 권장)~~  

```{r mean table}
mean_group<-data2 %>% mutate(age_group=
                            case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 ~ "60~")) %>% group_by(age_group) %>%
  summarise(mean(preg),mean(plas),mean(pres),mean(skin),mean(insu),mean(mass),mean(pedi)) %>% 
  select(-age_group) %>% round(3)



knitr::kable(mean_group)   # table form
```  

나이대별로 각 요인들의 평균치이다.  

```{r}
data3<-data2 %>% mutate(age_group=
                  case_when(age>=20 & age<30 ~ "20s",
                            age>=30 & age<40 ~ "30s",
                            age>=40 & age<50 ~ "40s",
                            age>=50 & age<60 ~ "50s",
                            age>=60 ~ "60~"))
```  

## EDA  

### Box-Plot {.tabset}

#### preg  

```{r, message=FALSE, warning=FALSE, echo=TRUE, fig.height=4.5, fig.width=9}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=preg))+ geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")  
```  

#### plas

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=plas))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")  
```  

#### pres  

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=pres))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")
```  

#### skin  

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=skin))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue") 
```  

#### insu  

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=insu))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")
```  

#### mass  

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=mass))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")
```  

#### pedi  

```{r}
data3[,!(names(data3) %in% c("class"))] %>% group_by(age_group) %>% 
  ggplot(aes(x=age_group,y=pedi))+geom_boxplot(outlier.color = 'red',outlier.shape = 2)+
  stat_summary(fun.y="mean", geom="point", shape=22, size=3, fill="blue")
```  
  
##

content below tabbed region  


```{r echo=FALSE}
data3$class_level<-ifelse(data3$class=="tested_negative",0,1)
datatable(data3,caption = 'Table : adjusted whole data',
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

```{r}
data3 %>% group_by(age_group) %>% 
  summarise(ratio=sum(class_level)/n()) %>%
  ggplot(aes(x=age_group,y=ratio))+geom_bar(stat="identity")
```  

당뇨병 환자들의 나이대들에 대한 비율 그래프이다. 위의 boxplot에서 plas(혈장 포도당 농도)의 그래프와 비슷한 양상을 보이고 있다. ~~plas가 당뇨병과 밀접한 관계가 있지 않을까?~~  

### Correlation Plot.  

```{r}
data3 %>% 
  select(-class, -age_group, -class_level) %>%
  select_if(is.numeric) %>%
  cor(use="complete.obs") %>%
  corrplot.mixed(tl.cex=0.85)
```  

(preg, age) (plas, insu) (skin, mass) 변수들이 어느정도 상관관계를 보이고 있다.  

## Machine Learning Algorithm. {.tabset}  

```{r}
set.seed(1234)
index<-createDataPartition(data3$class,p=0.7,list=F)

train<-data3[index,]
test<-data3[-index,]
```  

train과 test데이터를 70%확률로 랜덤하게 분리.  

```{r echo=FALSE}
datatable(train,caption = 'Table : train data',
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))

datatable(test, caption = "Table : test data",
          filter = 'top', options = list(pageLength = 10, autoWidth = TRUE))
```  

### Logistic Regression.  

```{r}
set.seed(1234)

logistic_model<-glm(class_level~preg+plas+pres+skin+insu+mass+pedi+age,
                    data = train, family = "binomial")

summary(logistic_model)
```  

```{r}
test$class_prob<-predict(logistic_model, test[,1:8],type = "response")

test$class_pred<-ifelse(test$class_prob>0.5,1,0)
```  

```{r}
mean(test$class_level==test$class_pred)
confusionMatrix(as.factor(test$class_pred),as.factor(test$class_level))
```  


```{r}
# In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of class_level you built earlier.  

ROC<-roc(test$class_level,test$class_prob)            # Create a ROC curve

plot(ROC,col="blue")                                  # Plot the ROC curve
auc(ROC)                                              # Calculate the area under the curve (AUC)
```  

The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). Classifiers that give curves closer to the top-left corner indicate a better performance.  

* refer  
[ROC1](https://nittaku.tistory.com/297)  
[ROC2](https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/)



### Decision Tree.  

```{r}
set.seed(1234)

tree_model <- rpart(class~preg+plas+pres+skin+insu+mass+pedi+age, data = train, method = "class", control = rpart.control(cp = 0))

tree_model
```  

```{r}
prediction<-predict(tree_model, test[,1:8], type = "class")

table(actual=test$class,pre=prediction); mean(prediction==test$class)
confusionMatrix(prediction,test$class)
```  

```{r}
# Plot the model with default settings
rpart.plot(tree_model)
```  

```{r}
# cross-validation을 계산해 주는 함수로 print.cp를 제공,rpart패키지도 과적합화 문제가 있기 때문
printcp(tree_model)
plotcp(tree_model)
```  

```{r}
ptree<-prune(tree_model, cp= tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)
```  

가지치기를 통해 과적합을 어느정도 줄인뒤, 다시 성능을 확인해보자.  

```{r}
rpartpred<-predict(ptree, test, type='class')
confusionMatrix(rpartpred, test$class)
```  


### RandomForest  

```{r}
set.seed(1234)

RandomForest_model<-randomForest(class~preg+plas+skin+insu+mass+pedi+age,data=train[,1:9],ntree=500,importance=T)
prediction2<-predict(RandomForest_model, test[,1:8], type = "class")
mean(prediction2==test$class)

confusionMatrix(prediction2, test$class)
```  

```{r}
importance(RandomForest_model)
varImpPlot(RandomForest_model, type=2, pch=19, col=1, cex=1, main="")
```  



```{r}
importance_data<-data.frame(importance(RandomForest_model))
importance_data[,5]<-rownames(importance_data)

ggplot(data=importance_data,aes(x=reorder(V5,-MeanDecreaseGini),y=MeanDecreaseGini))+
  geom_bar(stat="identity",fill="red")+
  theme_bw()
```  

변수의 중요도를 확인할 수 있다. 중요도가 낮은 하위 2개의 변수를 제외하고 다시 모델링을 해보자.  


```{r}
set.seed(1234)

RandomForest_model2<-randomForest(class~plas+insu+mass+pedi+age,data=train[,1:9],ntree=500,importance=T)
prediction3<-predict(RandomForest_model2, test[,1:8], type = "class")
mean(prediction3==test$class)

confusionMatrix(prediction3, test$class)
```  

성능이 개선된 것을 확인할 수 있다.  



```{r}
plot(RandomForest_model2$err.rate[,1],col='red')
```  


```{r}
#Even though random forest is so power full we accept the model only after cross validation

a<-trainControl(method="repeatedcv",number = 10,repeats = 10,index = createMultiFolds(train$class,k=10,times = 10))

rf<- train(x = train[,1:8], y = train[,9], method = "rf", tuneLength = 3,
              ntree = 500, trControl =a)

b<-predict(rf,test[,1:8])
confusionMatrix(b,test$class)
```  

### XGBOOST  

```{r}
# clf <- xgboost(data        = data.matrix(train[,1:8]), #matrix형을 input data로 활용
#                label       = train$class,
#                eta         = 0.025, #gradient descent 알고리즘에서의 learning rate
#                depth       = 10,
#                nrounds     = 1500, #maximum number of iterations (steps) required for gradient descent to converge. (?)
#                objective   = "reg:linear",
#                eval_metric = "rmse") #회귀모델에서는 RMSE를 모델 accuracy 평가지표로 활용 
```  


```{r}
set.seed(1234)

xgb<-xgboost(data = as.matrix(train[,1:8]), label = train$class_level, max.depth = 2, eta = 0.025, nthread = 2, nrounds = 100, objective = "binary:logistic")
```


```{r}
submission <- data.frame(Id=test$class)
submission$Response <-predict(xgb, data.matrix(test[,1:8]))

submission$result<-ifelse(submission$Response>mean(submission$Response),"tested_positive","tested_negative")

mean(submission$Id==submission$result)
```  

##

content below tabbed region  


## Evaluate Machine Learning Algorithms  

Method | Accuracy Performance  
-|-|  
Logistic Regression | 0.7652174  
Decision Tree | 0.7217391  
Random Forest | 0.7869565  
XGBOOST | 0.7565217  


Random Forest works good with 78.7% accuracy.  





